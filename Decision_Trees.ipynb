{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* entropy in decison tree\n",
    "* information gain, gini, chi square,\n",
    "* id3 \n",
    "* reduction in variance\n",
    "* pruning\n",
    "* handle overfitting\n",
    "* kaggle decision tree\n",
    "* enseble modelling\n",
    "* tree based ensemble models\n",
    "* cross validation (gridsearch)\n",
    "* decision tree regression\n",
    "* bagging, boosting\n",
    "* random forest\n",
    "* My model has more biased or more variance?\n",
    "* k-means k-nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Decision Tree is a white box type of ML algorithm.\n",
    "* CART - Classification and regression trees.\n",
    "\n",
    "A decision tree is a flowchart-like tree structure where an internal node represents feature(or attribute), the branch represents a decision rule, and each leaf node represents the outcome. The topmost node in a decision tree is known as the root node. It learns to partition on the basis of the attribute value. It partitions the tree in recursively manner call recursive partitioning. This flowchart-like structure helps you in decision making. It's visualization like a flowchart diagram which easily mimics the human level thinking. That is why decision trees are easy to understand and interpret.\n",
    "\n",
    "## Types of decision tree\n",
    "* **Categorical Variable Decision Tree**: Decision Tree which has categorical target variable then it called as categorical variable decision tree.\n",
    "* **Continuous Variable Decision Tree**: Decision Tree which has continuous target variable then it is called as Continuous Variable Decision Tree.\n",
    "\n",
    "## Advantages\n",
    "* It can easily capture Non-linear patterns.\n",
    "* It requires fewer data preprocessing from the user, for example, there is no need to normalize columns.\n",
    "* It can be used for feature engineering such as predicting missing values, suitable for variable selection.\n",
    "* The decision tree has no assumptions about distribution because of the non-parametric nature of the algorithm.\n",
    "\n",
    "## Disadvantages\n",
    "1. **Over fitting**: Over fitting is one of the most practical difficulty for decision tree models. This problem gets solved by setting constraints on model parameters and pruning (discussed in detailed below).\n",
    "2. **Not fit for continuous variables**: While working with continuous numerical variables, decision tree looses information when it categorizes variables in different categories.\n",
    "3. Sensitive to noisy data. It can overfit noisy data.\n",
    "4. The small variation(or variance) in data can result in the different decision tree. This can be reduced by bagging and boosting algorithms.\n",
    "5. Decision trees are biased with imbalance dataset, so it is recommended that balance out the dataset before creating the decision tree.\n",
    "\n",
    "## Attribute Selection Measures\n",
    "Attribute selection measure is a heuristic for selecting the splitting criterion that partition data into the best possible manner. It is also known as **splitting rules** because it helps us to determine breakpoints for tuples on a given node. ASM provides a rank to each feature(or attribute) by explaining the given dataset. Best score attribute will be selected as a splitting attribute (Source). In the case of a continuous-valued attribute, split points for branches also need to define.\n",
    "\n",
    "The decision of making strategic splits heavily affects a tree’s accuracy. The decision criteria is different for classification and regression trees.\n",
    "\n",
    "The creation of sub-nodes increases the homogeneity of resultant sub-nodes. In other words, we can say that purity of the node increases with respect to the target variable. Decision tree splits the nodes on all available variables and then selects the split which results in most homogeneous sub-nodes.\n",
    "\n",
    "## **What is entropy?**\n",
    "* https://towardsdatascience.com/entropy-how-decision-trees-make-decisions-2946b9c18c8\n",
    "\n",
    "Entry is measure of disorder in the data. If all the data is of same category then there is no disorder and entropy is 0.\n",
    "So entropy is less if data is homogeneous else it is high.\n",
    "\n",
    "The formula for entropy is -\n",
    "\n",
    "$$E(S) = \\sum_{i=1}^{c} - p_i \\log_2 p_i$$\n",
    "\n",
    "Here '$P_i$' is [frequentist probability](https://stats.stackexchange.com/questions/31867/bayesian-vs-frequentist-interpretations-of-probability) of an element or class 'i' in our data.\n",
    "\n",
    "Does it matter why entropy is measured using log base 2 or why entropy is measured between 0 and 1 and not some other range? No. It’s just a metric. It’s not important to know how it came to be. It’s important to know how to read it and what it tells us, which we just did above. Entropy is a measure of disorder or uncertainty and the goal of machine learning models and Data Scientists in general is to reduce uncertainty.\n",
    "\n",
    "Most popular selection measures are:\n",
    "* Information Gain (Entropy)\n",
    "* Gini\n",
    "* Chi-square\n",
    "* Reduction in variance\n",
    "\n",
    "Types\n",
    "* C4.5\n",
    "* ID3 (Entropy)\n",
    "\n",
    "#### Information gain\n",
    "\n",
    "\n",
    "* Search logistic regression vs decision trees\n",
    "* https://www.analyticsvidhya.com/blog/2016/04/complete-tutorial-tree-based-modeling-scratch-in-python/\n",
    "* http://www.saedsayad.com/decision_tree_reg.htm\n",
    "* https://www.datacamp.com/community/tutorials/decision-tree-classification-python\n",
    "* https://medium.com/@rishabhjain_22692/decision-trees-it-begins-here-93ff54ef134\n",
    "\n",
    "Evaluation\n",
    "* Accuracy\n",
    "* Precision\n",
    "* Recall\n",
    "* AUC\n",
    "* Log Loss\n",
    "* Categorical cross entropy\n",
    "* F1 score, F1 beta\n",
    "\n",
    "https://towardsdatascience.com/the-5-classification-evaluation-metrics-you-must-know-aa97784ff226\n",
    "\n",
    "Normalization should have no impact on the performance of a decision tree. It is generally useful, when you are solving a system of equations, least squares, etc, where you can have serious issues due to rounding errors. In decision tree, you are just comparing stuff and branching down the tree, so normalization would not help."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
