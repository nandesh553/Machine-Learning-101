{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What is feature normalization(feature scaling) and why is it needed in machine learning models?**\n",
    "\n",
    "Normalization is a preprocessing technique to standardize the data so that their absolute values come into same range. For machine learning, evry dataset does not require normalization. It is required only when fetures have different ranges.\n",
    "\n",
    "For example, consider a data set containing two features, age(x1), and income(x2). Where age ranges from 0–100, while income ranges from 0–20,000 and higher. Income is about 1,000 times larger than age and ranges from 20,000–500,000. So, these two features are in very different ranges. When we do further analysis, like multivariate linear regression, for example, the attributed income will intrinsically influence the result more due to its larger value. But this doesn’t necessarily mean it is more important as a predictor. This is where normlalization comes into play.\n",
    "\n",
    "**Why and Where to Apply Feature Scaling?**\n",
    "\n",
    "Real world dataset contains features that highly vary in magnitudes, units, and range. Normalisation should be performed when the scale of a feature is irrelevant or misleading and not should Normalise when the scale is meaningful.\n",
    "\n",
    "The algorithms which use Euclidean Distance measure are sensitive to Magnitudes. Here feature scaling helps to weigh all the features equally.\n",
    "\n",
    "Formally, If a feature in the dataset is big in scale compared to others then in algorithms where Euclidean distance is measured this big scaled feature becomes dominating and needs to be normalized.\n",
    "\n",
    "Examples of Algorithms where Feature Scaling matters\n",
    "1. K-Means : uses the Euclidean distance measure here feature scaling matters.\n",
    "2. K-Nearest-Neighbours : also require feature scaling.\n",
    "3. Principal Component Analysis (PCA): Tries to get the feature with maximum variance, here too feature scaling is required.\n",
    "4. Gradient Descent: Calculation speed increase as Theta calculation becomes faster after feature scaling.\n",
    "\n",
    "**Note: Naive Bayes, Linear Discriminant Analysis, and Tree-Based models are not affected by feature scaling.\n",
    "In Short, any Algorithm which is Not Distance based is Not affected by Feature Scaling.**\n",
    "\n",
    "* https://towardsdatascience.com/understand-data-normalization-in-machine-learning-8ff3062101f0\n",
    "* https://medium.com/@urvashilluniya/why-data-normalization-is-necessary-for-machine-learning-models-681b65a05029\n",
    "* https://www.geeksforgeeks.org/python-how-and-where-to-apply-feature-scaling/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formula used in Backend\n",
    "Standardisation replaces the values by their Z scores.\n",
    "\n",
    "\n",
    "<img src=\"images/standard.png\">\n",
    "\n",
    "\n",
    "Note, that not all algorithms behave this way. Certain types like Naive Bayes, Decision Trees, RF and XGB do not require feature scaling, because they work in a different manner.\n",
    "But, in general, algorithms that exploit distances or similarities (e.g. in form of scalar product) between data samples, such as k-NN and SVM, often require feature scaling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# X_train is the feature to be scaled\n",
    "# scaler = StandardScaler().fit(X_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://medium.com/@ian.dzindo01/feature-scaling-in-python-a59cc72147c1\n",
    "* http://benalexkeen.com/feature-scaling-with-scikit-learn/\n",
    "* https://www.analyticsvidhya.com/blog/2016/07/practical-guide-data-preprocessing-python-scikit-learn/\n",
    "\n",
    "The Standard Scaler is one of the most widely used scaling algorithms out there. It assumes that your data follows a Gaussian distribution (Gaussian distribution is the same thing as Normal distribution)\n",
    "The mean and the standard deviation are calculated for the feature and then the feature is scaled based on:\n",
    "(xi–mean(x))/stdev(x)\n",
    "The idea behind Standard Scaler is that it will transform your data, such that the distribution will have a mean value of 0 and a standard deviation of 1.\n",
    "\n",
    "<img src=\"images/standard_scaler.png\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
